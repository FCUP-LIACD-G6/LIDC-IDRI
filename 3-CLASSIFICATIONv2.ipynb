{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7567d7",
   "metadata": {},
   "source": [
    "# Neural Network approach 2\n",
    "\n",
    "#### -Testing a different approach, updating the entries labeled as 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42cad84",
   "metadata": {},
   "source": [
    "## - Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bb10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a7f14",
   "metadata": {},
   "source": [
    "## -Reading and defining the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e48349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "\n",
    "data = pd.read_csv(\"D:/DATASET LIDC-IDRI/processeddata/features.csv\")\n",
    "data = data.dropna()\n",
    "data=data.drop(['diagnostics_Mask-original_CenterOfMass','diagnostics_Mask-original_CenterOfMassIndex','diagnostics_Versions_PyRadiomics','diagnostics_Versions_Numpy','diagnostics_Versions_SimpleITK','diagnostics_Versions_PyWavelet','diagnostics_Versions_Python','diagnostics_Configuration_Settings','diagnostics_Configuration_EnabledImageTypes','diagnostics_Image-original_Hash','diagnostics_Image-original_Dimensionality','diagnostics_Image-original_Spacing','diagnostics_Image-original_Size','diagnostics_Image-original_Mean','diagnostics_Image-original_Minimum','diagnostics_Image-original_Maximum','diagnostics_Mask-original_Hash','diagnostics_Mask-original_Spacing','diagnostics_Mask-original_Size','diagnostics_Mask-original_BoundingBox','diagnostics_Mask-original_VolumeNum','Sid','Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195f081",
   "metadata": {},
   "source": [
    "### Instead of deleting the entries of mal=3 or mapping them all to 0 or 1, we had the idea to map the high 3's to 1 and the low 3's to 0. We did that by mapping the data into {1: 0, 2: 0, 4: 1, 5: 1} and sperating the data into the ones labeled as 0 and 1, and the ones labeled as 3. We trained the model on the binary dataset and predicted the dataset with our 3's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57ffdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating the data onto 01 and 3\n",
    "\n",
    "df = data\n",
    "df['malignancy_mapped'] = df['Malignancy'].map({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "df01 = df[df['malignancy_mapped'].isin([0, 1])]\n",
    "df3 = df[df['Malignancy'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed47ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and labels for the 0-1 labeled instances\n",
    "features_01 = df01.drop(['Malignancy', 'malignancy_mapped', 'Pid'], axis=1)\n",
    "labels_01 = df01['malignancy_mapped']\n",
    "\n",
    "# Separate the features for label 3 instances\n",
    "features_3 = df3.drop(['Malignancy', 'malignancy_mapped', 'Pid'], axis=1)\n",
    "\n",
    "# Train a model on 0-1 labeled instances\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_01, labels_01, test_size=0.2, random_state=50)\n",
    "model = LogisticRegression(max_iter=1000)  # You can use any model of your choice\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for label 3 instances\n",
    "probs = model.predict_proba(features_3)\n",
    "\n",
    "# Threshold to determine the 3+s and the 3-s\n",
    "threshold = 0.4885\n",
    "labels_3_mapped = np.where(probs[:, 1] >= threshold, 1, 0)\n",
    "mapped_labels_3 = pd.Series(labels_3_mapped, index=df3.index)\n",
    "\n",
    "# Combine all the labels\n",
    "final_labels = pd.concat([labels_01, mapped_labels_3], axis=0).sort_index()\n",
    "\n",
    "# Updating the dataset\n",
    "df = df.drop(['malignancy_mapped','Malignancy'],axis=1)\n",
    "df['Malv2'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2e3148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7186\n",
       "1.0    7162\n",
       "Name: Malv2, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Malv2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dbcad",
   "metadata": {},
   "source": [
    "## -Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c062f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting patients into groups\n",
    "\n",
    "all_patients = df['Pid'].unique() #732 (if we remove mal=3)\n",
    "\n",
    "# Spliting patients into train, test and val groups\n",
    "train_patients, test_patients = train_test_split(all_patients, test_size=0.2, random_state=50)\n",
    "\n",
    "# Creating the train, test and val datasets\n",
    "train_data = df[df['Pid'].isin(train_patients)]\n",
    "test_data = df[df['Pid'].isin(test_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6a1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "\n",
    "# Creating the train, test and val sets\n",
    "X_train = train_data.drop(['Pid','Malv2'], axis=1)\n",
    "X_test = test_data.drop(['Pid','Malv2'], axis=1)\n",
    "y_train = train_data['Malv2']\n",
    "y_test = test_data['Malv2']\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433cbbb",
   "metadata": {},
   "source": [
    "## -Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ae68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering (Random Forest)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=50)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the trained Random Forest model\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Sort features based on their importance\n",
    "feature_indices = feature_importances.argsort()[::-1]  # Sort in descending order\n",
    "\n",
    "# Select the top K important features\n",
    "k = 75\n",
    "top_k_features_indices = feature_indices[:k]\n",
    "\n",
    "# Filter your data to keep only the selected features\n",
    "X_train_selected = X_train[:, top_k_features_indices]\n",
    "X_test_selected = X_test[:, top_k_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ae20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the feature engineering\n",
    "\n",
    "X_train = X_train_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d52db",
   "metadata": {},
   "source": [
    "## -Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e97acaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "359/359 [==============================] - 2s 3ms/step - loss: 0.6231 - accuracy: 0.6763\n",
      "Epoch 2/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6916\n",
      "Epoch 3/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.6003 - accuracy: 0.6929\n",
      "Epoch 4/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5996 - accuracy: 0.6906\n",
      "Epoch 5/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5953 - accuracy: 0.6968\n",
      "Epoch 6/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5943 - accuracy: 0.6947\n",
      "Epoch 7/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5922 - accuracy: 0.6984\n",
      "Epoch 8/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5945 - accuracy: 0.6973\n",
      "Epoch 9/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5889 - accuracy: 0.6967\n",
      "Epoch 10/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5932 - accuracy: 0.6945\n",
      "Epoch 11/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5889 - accuracy: 0.6964\n",
      "Epoch 12/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5877 - accuracy: 0.6974\n",
      "Epoch 13/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5860 - accuracy: 0.6963\n",
      "Epoch 14/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5841 - accuracy: 0.7007\n",
      "Epoch 15/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5848 - accuracy: 0.6995\n",
      "Epoch 16/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5841 - accuracy: 0.6996\n",
      "Epoch 17/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5811 - accuracy: 0.6995\n",
      "Epoch 18/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5822 - accuracy: 0.6982\n",
      "Epoch 19/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.6995\n",
      "Epoch 20/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5790 - accuracy: 0.7019\n",
      "Epoch 21/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5790 - accuracy: 0.7004\n",
      "Epoch 22/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5793 - accuracy: 0.6992\n",
      "Epoch 23/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5760 - accuracy: 0.7022\n",
      "Epoch 24/30\n",
      "359/359 [==============================] - 1s 3ms/step - loss: 0.5821 - accuracy: 0.7007\n",
      "Epoch 25/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5746 - accuracy: 0.7036\n",
      "Epoch 26/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5771 - accuracy: 0.7028\n",
      "Epoch 27/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.7028\n",
      "Epoch 28/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5757 - accuracy: 0.7011\n",
      "Epoch 29/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5759 - accuracy: 0.7013\n",
      "Epoch 30/30\n",
      "359/359 [==============================] - 1s 2ms/step - loss: 0.5759 - accuracy: 0.7024\n"
     ]
    }
   ],
   "source": [
    "## NN aproach\n",
    "\n",
    "# Define the model\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "515983ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.7056\n",
      "Test Accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "## NN Results\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
